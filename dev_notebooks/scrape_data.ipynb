{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed article 1\n",
      "Processed article 2\n",
      "Processed article 3\n",
      "Processed article 4\n",
      "Processed article 5\n",
      "Processed article 6\n",
      "Processed article 7\n",
      "Processed article 8\n",
      "Processed article 9\n",
      "Processed article 10\n",
      "Processed article 11\n",
      "Processed article 12\n",
      "Processed article 13\n",
      "Processed article 14\n",
      "Processed article 15\n",
      "Processed article 16\n",
      "Processed article 17\n",
      "Processed article 18\n",
      "Processed article 19\n",
      "Processed article 20\n",
      "Processed article 21\n",
      "Processed article 22\n",
      "Processed article 23\n",
      "Processed article 24\n",
      "Processed article 25\n",
      "Processed article 26\n",
      "Processed article 27\n",
      "Processed article 28\n",
      "Processed article 29\n",
      "Processed article 30\n",
      "Processed article 31\n",
      "Processed article 32\n",
      "Processed article 33\n",
      "Processed article 34\n",
      "Processed article 35\n",
      "Processed article 36\n",
      "Processed article 37\n",
      "Processed article 38\n",
      "Processed article 39\n",
      "Processed article 40\n",
      "Processed article 41\n",
      "Processed article 42\n",
      "Processed article 43\n",
      "Processed article 44\n",
      "Processed article 45\n",
      "Processed article 46\n",
      "Processed article 47\n",
      "Processed article 48\n",
      "Processed article 49\n",
      "Processed article 50\n",
      "Processed article 51\n",
      "Processed article 52\n",
      "Processed article 53\n",
      "Processed article 54\n",
      "Processed article 55\n",
      "Processed article 56\n",
      "Processed article 57\n",
      "Processed article 58\n",
      "Processed article 59\n",
      "Processed article 60\n",
      "Processed article 61\n",
      "Processed article 62\n",
      "Processed article 63\n",
      "Processed article 64\n",
      "Processed article 65\n",
      "Processed article 66\n",
      "Processed article 67\n",
      "Processed article 68\n",
      "Processed article 69\n",
      "Processed article 70\n",
      "Processed article 71\n",
      "Processed article 72\n",
      "Processed article 73\n",
      "Processed article 74\n",
      "Processed article 75\n",
      "Processed article 76\n",
      "Processed article 77\n",
      "Processed article 78\n",
      "Processed article 79\n",
      "Processed article 80\n",
      "Processed article 81\n",
      "Processed article 82\n",
      "Processed article 83\n",
      "Processed article 84\n",
      "Processed article 85\n",
      "Processed article 86\n",
      "Processed article 87\n",
      "Processed article 88\n",
      "Processed article 89\n",
      "Processed article 90\n",
      "Processed article 91\n",
      "Processed article 92\n",
      "Processed article 93\n",
      "Processed article 94\n",
      "Processed article 95\n",
      "Processed article 96\n",
      "Processed article 97\n",
      "Processed article 98\n",
      "Processed article 99\n",
      "Processed article 100\n",
      "Processed article 101\n",
      "Processed article 102\n",
      "Processed article 103\n",
      "Processed article 104\n",
      "Processed article 105\n",
      "Processed article 106\n",
      "Processed article 107\n",
      "Processed article 108\n",
      "Processed article 109\n",
      "Processed article 110\n",
      "Processed article 111\n",
      "Processed article 112\n",
      "Processed article 113\n"
     ]
    }
   ],
   "source": [
    "all_articles = []\n",
    "\n",
    "for i in range(1,114):\n",
    "    url = f\"https://artificialintelligenceact.eu/article/{i}/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    summary = soup.find(\"div\", class_=\"aia-clairk-summary-content-section\").text\n",
    "    div_part_of = soup.find('div', class_='aia-post-meta-wrapper')\n",
    "    part_of = div_part_of.find('a').text\n",
    "    title = soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    sub_title_info = soup.find(\"div\", class_=\"aia-eif-wrapper\").text\n",
    "    full_text = soup.find(\"div\", class_=\"et_pb_module et_pb_post_content et_pb_post_content_0_tb_body\").text\n",
    "    try:\n",
    "        related_recitals = soup.find(\"div\", class_=\"aia-explore-related-list\").text\n",
    "        related_recitals = \"\\n\".join([line for line in related_recitals.split(\"\\n\") if line.strip()])\n",
    "    except:\n",
    "        related_recitals = None\n",
    "    all_articles.append({\n",
    "        \"summary\": summary,\n",
    "        \"part_of\": part_of,\n",
    "        \"title\": title,\n",
    "        \"sub_title_info\": sub_title_info,\n",
    "        \"full_text\": full_text,\n",
    "        \"related_recitals\": related_recitals\n",
    "    })\n",
    "    print(f\"Processed article {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': '\\n The EU can fine companies that provide general-purpose AI models up to 3% of their annual global turnover or â‚¬15 million, whichever is higher, if they break the rules. This could be for not following regulations, not providing requested documents or information, not complying with certain measures, or not allowing the EU to access their AI model for evaluation. The severity of the fine depends on the seriousness and length of the infringement. Companies will be told about any preliminary findings and given a chance to respond before a decision is made. The EU Court of Justice can review and change any fines.\\n\\nGenerated by CLaiRK, edited by us.\\n',\n",
       " 'part_of': 'Chapter XII: Penalties',\n",
       " 'title': 'Article 101: Fines for Providers of General-Purpose AI Models',\n",
       " 'sub_title_info': '\\n\\nDate of entry into force:\\n 2 August 2026\\n\\n\\nAccording to:\\n Article 113\\n\\n',\n",
       " 'full_text': '\\n1. The Commission may impose on providers of general-purpose AI models fines not exceeding 3 % of their annual total worldwide turnover in the preceding financial year or EUR 15 000 000, whichever is higher., when the Commission finds that the provider intentionally or negligently:\\n(a) infringed the relevant provisions of this Regulation;\\n(b) failed to comply with a request for a document or for information pursuant to Article 91, or supplied incorrect, incomplete or misleading information;\\n(c) failed to comply with a measure requested under Article 93;\\n(d) failed to make available to the Commission access to the general-purpose AI model or general-purpose AI model with systemic risk with a view to conducting an evaluation pursuant to Article 92. In fixing the amount of the fine or periodic penalty payment, regard shall be had to the nature, gravity and duration of the infringement, taking due account of the principles of proportionality and appropriateness. The Commission shall also into account commitments made in accordance with Article 93(3) or made in relevant codes of practice in accordance with Article 56.\\n2. Before adopting the decision pursuant to paragraph 1, the Commission shall communicate its preliminary findings to the provider of the general-purpose AI model and give it an opportunity to be heard.\\n3. Fines imposed in accordance with this Article shall be effective, proportionate and dissuasive.\\n4. Information on fines imposed under this Article shall also be communicated to the Board as appropriate.\\n5. The Court of Justice of the European Union shall have unlimited jurisdiction to review decisions of the Commission fixing a fine under this Article. It may cancel, reduce or increase the fine imposed.\\n6. The Commission shall adopt implementing acts containing detailed arrangements and procedural safeguards for proceedings in view of the possible adoption of decisions pursuant to paragraph 1 of this Article. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2).\\n',\n",
       " 'related_recitals': 'Suitable Recitals\\n169'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed recital 1\n",
      "Processed recital 2\n",
      "Processed recital 3\n",
      "Processed recital 4\n",
      "Processed recital 5\n",
      "Processed recital 6\n",
      "Processed recital 7\n",
      "Processed recital 8\n",
      "Processed recital 9\n",
      "Processed recital 10\n",
      "Processed recital 11\n",
      "Processed recital 12\n",
      "Processed recital 13\n",
      "Processed recital 14\n",
      "Processed recital 15\n",
      "Processed recital 16\n",
      "Processed recital 17\n",
      "Processed recital 18\n",
      "Processed recital 19\n",
      "Processed recital 20\n",
      "Processed recital 21\n",
      "Processed recital 22\n",
      "Processed recital 23\n",
      "Processed recital 24\n",
      "Processed recital 25\n",
      "Processed recital 26\n",
      "Processed recital 27\n",
      "Processed recital 28\n",
      "Processed recital 29\n",
      "Processed recital 30\n",
      "Processed recital 31\n",
      "Processed recital 32\n",
      "Processed recital 33\n",
      "Processed recital 34\n",
      "Processed recital 35\n",
      "Processed recital 36\n",
      "Processed recital 37\n",
      "Processed recital 38\n",
      "Processed recital 39\n",
      "Processed recital 40\n",
      "Processed recital 41\n",
      "Processed recital 42\n",
      "Processed recital 43\n",
      "Processed recital 44\n",
      "Processed recital 45\n",
      "Processed recital 46\n",
      "Processed recital 47\n",
      "Processed recital 48\n",
      "Processed recital 49\n",
      "Processed recital 50\n",
      "Processed recital 51\n",
      "Processed recital 52\n",
      "Processed recital 53\n",
      "Processed recital 54\n",
      "Processed recital 55\n",
      "Processed recital 56\n",
      "Processed recital 57\n",
      "Processed recital 58\n",
      "Processed recital 59\n",
      "Processed recital 60\n",
      "Processed recital 61\n",
      "Processed recital 62\n",
      "Processed recital 63\n",
      "Processed recital 64\n",
      "Processed recital 65\n",
      "Processed recital 66\n",
      "Processed recital 67\n",
      "Processed recital 68\n",
      "Processed recital 69\n",
      "Processed recital 70\n",
      "Processed recital 71\n",
      "Processed recital 72\n",
      "Processed recital 73\n",
      "Processed recital 74\n",
      "Processed recital 75\n",
      "Processed recital 76\n",
      "Processed recital 77\n",
      "Processed recital 78\n",
      "Processed recital 79\n",
      "Processed recital 80\n",
      "Processed recital 81\n",
      "Processed recital 82\n",
      "Processed recital 83\n",
      "Processed recital 84\n",
      "Processed recital 85\n",
      "Processed recital 86\n",
      "Processed recital 87\n",
      "Processed recital 88\n",
      "Processed recital 89\n",
      "Processed recital 90\n",
      "Processed recital 91\n",
      "Processed recital 92\n",
      "Processed recital 93\n",
      "Processed recital 94\n",
      "Processed recital 95\n",
      "Processed recital 96\n",
      "Processed recital 97\n",
      "Processed recital 98\n",
      "Processed recital 99\n",
      "Processed recital 100\n",
      "Processed recital 101\n",
      "Processed recital 102\n",
      "Processed recital 103\n",
      "Processed recital 104\n",
      "Processed recital 105\n",
      "Processed recital 106\n",
      "Processed recital 107\n",
      "Processed recital 108\n",
      "Processed recital 109\n",
      "Processed recital 110\n",
      "Processed recital 111\n",
      "Processed recital 112\n",
      "Processed recital 113\n",
      "Processed recital 114\n",
      "Processed recital 115\n",
      "Processed recital 116\n",
      "Processed recital 117\n",
      "Processed recital 118\n",
      "Processed recital 119\n",
      "Processed recital 120\n",
      "Processed recital 121\n",
      "Processed recital 122\n",
      "Processed recital 123\n",
      "Processed recital 124\n",
      "Processed recital 125\n",
      "Processed recital 126\n",
      "Processed recital 127\n",
      "Processed recital 128\n",
      "Processed recital 129\n",
      "Processed recital 130\n",
      "Processed recital 131\n",
      "Processed recital 132\n",
      "Processed recital 133\n",
      "Processed recital 134\n",
      "Processed recital 135\n",
      "Processed recital 136\n",
      "Processed recital 137\n",
      "Processed recital 138\n",
      "Processed recital 139\n",
      "Processed recital 140\n",
      "Processed recital 141\n",
      "Processed recital 142\n",
      "Processed recital 143\n",
      "Processed recital 144\n",
      "Processed recital 145\n",
      "Processed recital 146\n",
      "Processed recital 147\n",
      "Processed recital 148\n",
      "Processed recital 149\n",
      "Processed recital 150\n",
      "Processed recital 151\n",
      "Processed recital 152\n",
      "Processed recital 153\n",
      "Processed recital 154\n",
      "Processed recital 155\n",
      "Processed recital 156\n",
      "Processed recital 157\n",
      "Processed recital 158\n",
      "Processed recital 159\n",
      "Processed recital 160\n",
      "Processed recital 161\n",
      "Processed recital 162\n",
      "Processed recital 163\n",
      "Processed recital 164\n",
      "Processed recital 165\n",
      "Processed recital 166\n",
      "Processed recital 167\n",
      "Processed recital 168\n",
      "Processed recital 169\n",
      "Processed recital 170\n",
      "Processed recital 171\n",
      "Processed recital 172\n",
      "Processed recital 173\n",
      "Processed recital 174\n",
      "Processed recital 175\n",
      "Processed recital 176\n",
      "Processed recital 177\n",
      "Processed recital 178\n",
      "Processed recital 179\n",
      "Processed recital 180\n"
     ]
    }
   ],
   "source": [
    "all_recitals = []\n",
    "\n",
    "for i in range(1,181):\n",
    "    url = f\"https://artificialintelligenceact.eu/recital/{i}/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    # summary = soup.find(\"div\", class_=\"aia-clairk-summary-content-section\").text\n",
    "    # div_part_of = soup.find('div', class_='aia-post-meta-wrapper')\n",
    "    # part_of = div_part_of.find('a').text\n",
    "    title = soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    # sub_title_info = soup.find(\"div\", class_=\"aia-eif-wrapper\").text\n",
    "    full_text = soup.find(\"div\", class_=\"et_pb_module et_pb_post_content et_pb_post_content_0_tb_body\").text\n",
    "    try:\n",
    "        related_content = soup.find(\"div\", class_=\"aia-explore-related-list\").text\n",
    "        related_content = \"\\n\".join([line for line in related_content.split(\"\\n\") if line.strip()])\n",
    "    except:\n",
    "        related_content = None\n",
    "    all_recitals.append({\n",
    "        \"title\": title,\n",
    "        \"full_text\": full_text,\n",
    "        \"related_content\": related_content\n",
    "    })\n",
    "    print(f\"Processed recital {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed annex 1\n",
      "Processed annex 2\n",
      "Processed annex 3\n",
      "Processed annex 4\n",
      "Processed annex 5\n",
      "Processed annex 6\n",
      "Processed annex 7\n",
      "Processed annex 8\n",
      "Processed annex 9\n",
      "Processed annex 10\n",
      "Processed annex 11\n",
      "Processed annex 12\n",
      "Processed annex 13\n"
     ]
    }
   ],
   "source": [
    "all_annexes = []\n",
    "\n",
    "for i in range(1,14):\n",
    "    url = f\"https://artificialintelligenceact.eu/annex/{i}/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    summary = soup.find(\"div\", class_=\"aia-clairk-summary-content-section\").text\n",
    "    title = soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    full_text = soup.find(\"div\", class_=\"et_pb_module et_pb_post_content et_pb_post_content_0_tb_body\").text\n",
    "    try:\n",
    "        related_recitals = soup.find(\"div\", class_=\"aia-explore-related-list\").text\n",
    "        related_recitals = \"\\n\".join([line for line in related_recitals.split(\"\\n\") if line.strip()])\n",
    "    except:\n",
    "        related_recitals = None\n",
    "    \n",
    "    all_annexes.append({\n",
    "        \"summary\": summary,\n",
    "        \"title\": title,\n",
    "        \"full_text\": full_text,\n",
    "        \"related_recitals\": related_recitals\n",
    "    })\n",
    "    print(f\"Process ed annex {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all to a csv\n",
    "import pandas as pd\n",
    "\n",
    "df_articles = pd.DataFrame(all_articles)\n",
    "df_articles.to_csv(\"articles.csv\", index=False)\n",
    "\n",
    "df_recitals = pd.DataFrame(all_recitals)\n",
    "df_recitals.to_csv(\"recitals.csv\", index=False)\n",
    "\n",
    "df_annexes = pd.DataFrame(all_annexes)\n",
    "df_annexes.to_csv(\"annexes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = \"\"\n",
    "\n",
    "all_articles = \"\"\n",
    "\n",
    "for i in range(1,114):\n",
    "    url = f\"https://artificialintelligenceact.eu/article/{i}/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    summary = soup.find(\"div\", class_=\"aia-clairk-summary-content-section\").text\n",
    "    div_part_of = soup.find('div', class_='aia-post-meta-wrapper')\n",
    "    part_of = div_part_of.find('a').text\n",
    "    title = soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    sub_title_info = soup.find(\"div\", class_=\"aia-eif-wrapper\").text\n",
    "    full_text = soup.find(\"div\", class_=\"et_pb_module et_pb_post_content et_pb_post_content_0_tb_body\").text\n",
    "    try:\n",
    "        related_recitals = soup.find(\"div\", class_=\"aia-explore-related-list\").text\n",
    "        related_recitals = \"\\n\".join([line for line in related_recitals.split(\"\\n\") if line.strip()])\n",
    "    except:\n",
    "        related_recitals = None\n",
    "    all_articles += f\"{title}\\n{sub_title_info}\\n{summary}\\n{part_of}\\n{full_text}\\n{related_recitals}\\n\"\n",
    "    print(f\"Processed article {i}\")\n",
    "\n",
    "all_recitals = \"\"\n",
    "\n",
    "for i in range(1,181):\n",
    "    url = f\"https://artificialintelligenceact.eu/recital/{i}/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    title = soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    full_text = soup.find(\"div\", class_=\"et_pb_module et_pb_post_content et_pb_post_content_0_tb_body\").text\n",
    "    try:\n",
    "        related_content = soup.find(\"div\", class_=\"aia-explore-related-list\").text\n",
    "        related_content = \"\\n\".join([line for line in related_content.split(\"\\n\") if line.strip()])\n",
    "    except:\n",
    "        related_content = None\n",
    "    all_recitals += f\"{title}\\n{full_text}\\n{related_content}\\n\"\n",
    "    print(f\"Processed recital {i}\")\n",
    "\n",
    "all_annexes = \"\"\n",
    "\n",
    "for i in range(1,14):\n",
    "    url = f\"https://artificialintelligenceact.eu/annex/{i}/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    summary = soup.find(\"div\", class_=\"aia-clairk-summary-content-section\").text\n",
    "    title = soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    full_text = soup.find(\"div\", class_=\"et_pb_module et_pb_post_content et_pb_post_content_0_tb_body\").text\n",
    "    try:\n",
    "        related_recitals = soup.find(\"div\", class_=\"aia-explore-related-list\").text\n",
    "        related_recitals = \"\\n\".join([line for line in related_recitals.split(\"\\n\") if line.strip()])\n",
    "    except:\n",
    "        related_recitals = None\n",
    "    \n",
    "    all_annexes += f\"{title}\\n{summary}\\n{full_text}\\n{related_recitals}\\n\"\n",
    "    print(f\"Processed annex {i}\")\n",
    "\n",
    "all_texts = all_articles + all_recitals + all_annexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annexes['token_count'] = annexes['full_text'].apply(lambda x: len(x.split()) / 4)\n",
    "articles['token_count'] = articles['full_text'].apply(lambda x: len(x.split()) / 4)\n",
    "recitals['token_count'] = recitals['full_text'].apply(lambda x: len(x.split()) / 4)\n",
    "\n",
    "# show each token_count stats\n",
    "print(annexes['token_count'].describe())\n",
    "print(articles['token_count'].describe())\n",
    "print(recitals['token_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "annexes['mentioned_articles'] = annexes['full_text'].apply(lambda x: re.findall(r'Article\\s\\d+', x))\n",
    "annexes['mentioned_annexes'] = annexes['full_text'].apply(lambda x: re.findall(r'Annex\\s[IVXLCDM]+', x))\n",
    "annexes['mentioned_recitals'] = annexes['full_text'].apply(lambda x: re.findall(r'Recital\\s\\d+', x))\n",
    "\n",
    "# do the same for articles and recitals\n",
    "articles['mentioned_articles'] = articles['full_text'].apply(lambda x: re.findall(r'Article\\s\\d+', x))\n",
    "articles['mentioned_annexes'] = articles['full_text'].apply(lambda x: re.findall(r'Annex\\s[IVXLCDM]+', x))\n",
    "articles['mentioned_recitals'] = articles['full_text'].apply(lambda x: re.findall(r'Recital\\s\\d+', x))\n",
    "\n",
    "recitals['mentioned_articles'] = recitals['full_text'].apply(lambda x: re.findall(r'Article\\s\\d+', x))\n",
    "recitals['mentioned_annexes'] = recitals['full_text'].apply(lambda x: re.findall(r'Annex\\s[IVXLCDM]+', x))\n",
    "recitals['mentioned_recitals'] = recitals['full_text'].apply(lambda x: re.findall(r'Recital\\s\\d+', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annexes['type'] = 'annex'\n",
    "articles['type'] = 'article'\n",
    "recitals['type'] = 'recital'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recitals['related_content'] = recitals['related_content'].apply(lambda x: x.split('\\n')[1:] if type(x) == str else x)\n",
    "articles['related_recitals'] = articles['related_recitals'].apply(lambda x: x.split('\\n')[1:] if type(x) == str else x)\n",
    "annexes['related_recitals'] = annexes['related_recitals'].apply(lambda x: x.split('\\n')[1:] if type(x) == str else x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
